Scrapen (Engels: web scraping) is een computertechniek waarbij software wordt gebruikt om informatie van webpagina's te extraheren en al dan niet te analyseren. Meestal probeert de software een deel van het world wide web te onderzoeken via gebruik van het op codes gebaseerde Hypertext Transfer Protocol (HTTP), of door het surfgedrag met een webbrowser zoals Mozilla Firefox te simuleren.
Scrapen is sterk gerelateerd aan web-indexering, waarmee een bot of een web-crawler op een automatische manier de informatie verzamelt en categoriseert, een techniek die universeel wordt toegepast door de meeste zoekmachines. Scrapen daarentegen focust zich vooral op de de omzetting van ongestructureerde data, meestal in HTML-formaat, naar gestructureerde data die kan worden bewaard en geanalyseerd in een centrale lokale database of spreadsheet.
Behalve door zoekmachines wordt de techniek ook vaak ingezet voor het vergaren van data wanneer de aanbieder ervan deze niet op een gestructureerde manier weggeeft